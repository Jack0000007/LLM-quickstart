      我是一名开发工程师，在公司目前主要负责大语言模型智能问答系统的开发工作。一开始都只是应用langchain来做RAG做公司所属特定领域的大模型智能问答系统。外挂知识库尽管可以减少一定程度的幻觉，但是通用开源大模型对于特定业务场景下，模型仅仅通过提示词优化跟知识库增强还是无法在可以接受的误差范围内解决问题。所以就想着用微调的方式来进行解析用户提出的问题。我们方案是结合function call的方式去捕获关键词然后去调用业务接口执行查询得到结果。之前对于大模型由于工作应用我自学过微调技术，但是学的不够全面，通过这次训练营课程我更加全面的学习到了微调的方法技巧以及了解到了微调适用范围，也初步针对公司业务场景做了微调尝试取得了初步的成果。在此感谢极客时间的课程让我有了一定的技术提升。
      在极客时间训练营的AI大模型微调训练营中，我经历了一段充实而富有成效的学习旅程。以下是我在这门课程中所学到的知识点和内容的总结：
1. 大模型微调的重要性
我深刻理解了在构建AI应用时，对大模型进行微调的必要性。微调是提升模型在特定任务上性能的关键步骤，它使得模型能够更加精准地适应各种应用场景。
2. 主流微调技术
通过课程，我掌握了以下主流的微调技术：
全量微调：通过调整所有参数，使模型更好地适应特定任务。
模型量化：使用GPTQ和AWQ技术对模型进行量化，以减少模型大小和提高运算效率。
LORA微调：一种轻量级的微调方法，适用于Whisper-Large-v2等中文语音识别模型。
QLoRA微调：在ChatGLM3-6B等大型模型上进行微调，以提高模型在特定任务上的表现。
3. 实战训练
在实战训练中，我有机会亲自操作并优化以下模型：
BERT模型：通过全量微调，提升了模型在自然语言处理任务上的性能。
Whisper-Large-v2：使用LORA技术进行了语音识别的微调。
ChatGLM3：通过私有数据微调，使模型更好地适应特定的对话场景。
LLaMA2-7B：进行了指令微调，提高了模型对用户指令的理解能力。
4. 分布式训练
学习了如何使用DeepSpeed ZeRO-2和ZeRO-3技术进行分布式训练，这对于处理大规模模型尤为重要，有效提升了训练效率和模型性能。